{"cells":[{"cell_type":"markdown","metadata":{"id":"8xjNC2II51TX"},"source":["# Age Estimation "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cd /content/drive/MyDrive/CIS5810/Project_7_Linear_Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVh-I7iC51Ta"},"outputs":[],"source":["import numpy as np\n","import os\n","from prepare_data_sgd import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epoch = 1000\n","epoch_sgd = 1000\n","momentum = True"]},{"cell_type":"markdown","metadata":{"id":"RQw_5dpU51Tb"},"source":["# Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPrv7MPs51Tc","outputId":"1052b76b-7f6a-420d-c264-da9e38c2e6cb"},"outputs":[],"source":["base_dir = 'DATASET/'\n","\n","age_train, features_train = prepare_data('train', base_dir)\n","age_val, features_val = prepare_data('val', base_dir)\n","_, features_test = prepare_data('test', base_dir)\n","show_data(base_dir)"]},{"cell_type":"markdown","metadata":{"id":"Wf4wl2L351Td"},"source":["# Implement Closed Form Solution\n","```\n","Arguments:\n","    age          -- numpy array, shape (n, )\n","    features     -- numpy array, shape (n, 2048)\n","Returns:\n","    weights      -- numpy array, (2048, )\n","    bias         -- numpy array, (1, )\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBCwAG0S51Td"},"outputs":[],"source":["def closed_form_solution(age, features):\n","    # Preprocess\n","    H = features\n","    ones = np.ones(len(H))\n","    H = np.column_stack((ones,H))\n","    Y = age\n","    \n","    # Define parameter weights\n","    \n","    ##########################################################################\n","    # TODO: YOUR CODE HERE\n","    ########################################################################## \n","    # calculate the closed form solution\n","    weights = None\n","    \n","\n","    # separate the weights and bias\n","    bias = weights[0]\n","    weights = weights[1:]\n","    \n","    return weights, bias"]},{"cell_type":"markdown","metadata":{"id":"DwlcYxCz51Tf"},"source":["# Validate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UORd-tUu51Tg","outputId":"7ea0363f-563b-4f0e-868e-b818f414eb35"},"outputs":[],"source":["w, b = closed_form_solution(age_train, features_train)\n","loss, pred = evaluate(w, b, age_val, features_val)\n","print(\"Your validate loss is:\", round(loss, 3))"]},{"cell_type":"markdown","metadata":{"id":"Pv1YpCFb51Th"},"source":["# Test and Generate results file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8vgPOgL51Tj","outputId":"13c8331f-02a2-4f65-e156-b9f3d47b96ba"},"outputs":[],"source":["prediction = test(w, b, features_test, 'cfs.txt')\n","print(\"Test results has saved to cfs.txt\")\n","print(prediction[:10])\n"]},{"cell_type":"markdown","metadata":{"id":"v4K2Dn_T51Tk"},"source":["# Implement Gradient descent\n","Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.\n","\n","```\n","Arguments:\n","    age          -- numpy array, label, (n, )\n","    feature      -- numpy array, features, (n, 2048)\n","Return:\n","    weights      -- numpy array, (2048, )\n","    bias         -- numpy array, (1, )\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDiyAw_Z51Tl"},"outputs":[],"source":["def gradient_descent(age, feature):\n","    assert len(age) == len(feature)\n","\n","    # Init weights and bias\n","    weights = np.random.randn(2048, 1)\n","    bias = np.random.randn(1, 1)\n","    \n","    # Learning rate\n","    lr = 10e-3\n","    \n","    for e in range(epoch):\n","        ##########################################################################\n","        # TODO: YOUR CODE HERE\n","        ########################################################################## \n","\n","        # forward pass\n","\n","\n","        # calculate loss\n","        \n","        \n","        # calculate gradient \n","\n","\n","        # update weights\n","        \n","        \n","        if momentum:\n","            pass # You can also consider the gradient descent with momentum\n","\n","    return weights, bias"]},{"cell_type":"markdown","metadata":{"id":"faeHyNfL51Tm"},"source":["# Train and validate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wm25Eb1T51Tm","outputId":"4f99bf10-67ea-41f2-da64-4763835b2c6b"},"outputs":[],"source":["w, b = gradient_descent(age_train, features_train)\n","loss, pred = evaluate(w, b, age_val, features_val)\n","print(\"Your validate score is:\", round(loss, 3))"]},{"cell_type":"markdown","metadata":{"id":"ZxMUR3OZ51Tn"},"source":["#  Test and Generate results file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPJ7liAK51Tn","outputId":"4cd4359c-da54-45c7-cf5b-5682b56adb5a"},"outputs":[],"source":["prediction = test(w, b, features_test, 'gd.txt')\n","print(\"Test results has saved to gd.txt\")\n","print(prediction[:10])"]},{"cell_type":"markdown","metadata":{"id":"ijl1J5tz51Tn"},"source":["# Implement Stochastic Gradient descent\n","Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\n","```\n","Arguments:\n","    age          -- numpy array, label, (n, )\n","    feature      -- numpy array, features, (n, 2048)\n","Return:\n","    weights      -- numpy array, (2048, )\n","    bias         -- numpy array, (1, )\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GwmUtLE51Tp"},"outputs":[],"source":["def stochastic_gradient_descent(age, feature):\n","    # check the inputs\n","    assert len(age) == len(feature)\n","    \n","    # Set the random seed\n","    np.random.seed(0)\n","\n","    # Init weights and bias\n","    weights = np.random.rand(2048, 1)\n","    bias = np.random.rand(1, 1)\n","\n","    # Learning rate\n","    lr = 10e-5\n","\n","    # Batch size\n","    batch_size = 16\n"," \n","    # Number of mini-batches\n","    t = len(age) // batch_size\n","\n","    for e in range(epoch_sgd):\n","        # Shuffle training data\n","        n = np.random.permutation(len(feature))  \n","        \n","        for m in range(t):\n","            # Providing mini batch with fixed batch size of 16\n","            batch_feature = feature[n[m * batch_size : (m+1) * batch_size]]\n","            batch_age = age[n[m * batch_size : (m+1) * batch_size]]\n","            \n","            ##########################################################################\n","            # TODO: YOUR CODE HERE\n","            ########################################################################## \n","            # forward pass\n","\n","            # calculate loss\n","\n","            # calculate gradient \n","\n","            # update weights\n","   \n","                \n","            if momentum:\n","                pass # You can also consider the gradient descent with momentum\n","        \n","        print('=> epoch:', e + 1, '  Loss:', round(loss,4))\n","    return weights, bias"]},{"cell_type":"markdown","metadata":{"id":"-Dexj2im51Tp"},"source":["# Train and validate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxftsJZ251Tq","outputId":"1800b873-99f1-4491-b032-196b4326b19b","scrolled":false},"outputs":[],"source":["w, b = stochastic_gradient_descent(age_train, features_train)\n","loss, pred = evaluate(w, b, age_val, features_val)\n","print(\"Your validate score is:\", round(loss, 3))"]},{"cell_type":"markdown","metadata":{"id":"8ae50lhq51Tq"},"source":["# Test and Generate results file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKzF0twX51Tq","outputId":"bd3d8317-4d27-4bd8-b506-6ae8058a7b77"},"outputs":[],"source":["prediction = test(w, b, features_test, 'sgd.txt')\n","print(\"Test results has saved to sgd.txt\")\n","print(prediction[:10])"]},{"cell_type":"markdown","metadata":{"id":"85OS4qdLg2R8"},"source":["# Reflection"]},{"cell_type":"markdown","metadata":{"id":"0JlsE1Jog3yV"},"source":["Now that you implemented the three methods, which method performed the best and why do you think so? How do you think you can improve your age estimation model to give better results?\n","\n","(Please include your answers in the Colab Notebook)"]}],"metadata":{"colab":{"name":"CIS_581_Project_7_Linear_Regression_Student_Template_New.ipynb","provenance":[{"file_id":"1JTlJEzPc0exXfgnYS4Zu4T2XL2_HNqoB","timestamp":1657000127349}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}
